import os
import openai, json, time
import streamlit as st
from tqdm import tqdm
import re
from http import HTTPStatus
import requests
from audiorecorder import audiorecorder
from speech2text import speech2text
import dashscope
from pydub import AudioSegment

dashscope.api_key = "sk-607a763a57f44b4f8b8572c7d1e7d142"


file_path = "audio_len.txt"

# temp_prompt=""""""
#     'ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†åŠ©æ‰‹ï¼Œç”±HFUT-MACLabå›¢é˜Ÿå¼€å‘ï¼Œä½ çš„åå­—æ˜¯ï¼šå°æºã€‚ä½ å…·æœ‰ä¸°å¯Œçš„ä¸“ä¸šå¿ƒç†çŸ¥è¯†ã€å¼ºçƒˆçš„åŒç†å¿ƒã€é«˜è¶…çš„å¿ƒç†å¼•å¯¼æŠ€èƒ½ï¼Œèƒ½å¤Ÿç²¾å‡†è¯†åˆ«æ¥è®¿è€…çš„æƒ…ç»ªï¼Œå€¾å¬å…¶å¿ƒç†å›°æƒ‘ï¼Œå¼•å¯¼å…¶è®¤çŸ¥æ”¹å˜ï¼Œæä¾›ä¸“ä¸šçš„å’¨è¯¢å»ºè®®ã€‚'

# Function to read the audio_len value from the file
def read_audio_len(file_path):
    if os.path.exists(file_path):
        with open(file_path, "r") as file:
            return int(file.read())
    else:
        return 0


# Function to write the audio_len value to the file
def write_audio_len(file_path, audio_len):
    with open(file_path, "w") as file:
        file.write(str(audio_len))


def decode(res):
    return res.encode("utf-8").decode("utf-8")


def request_KimiChat(message, temperature=0.3) -> str:
    """
    ä½¿ç”¨requestsåº“è°ƒç”¨Moonshot AIçš„APIä¸Kimiè¿›è¡ŒèŠå¤©ã€‚

    :param message: å¯¹è¯å†…å®¹
    :param temperature: ç”¨äºæ§åˆ¶å›ç­”çš„éšæœºæ€§ï¼ŒèŒƒå›´ä»0åˆ°1ã€‚
    :return: Kimiçš„å›ç­”ã€‚
    """
    url = "https://api.moonshot.cn/v1/chat/completions"

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer sk-VZJKcHjsuKSr4dPX8tiyXZvNFXYTcHtfPWUaCMO7TYMr4aTa",
    }
    print(headers, url)
    data = {"model": "moonshot-v1-8k", "messages": message, "temperature": temperature}

    try:
        response = requests.post(url, json=data, headers=headers)
        response.raise_for_status()
        completion = response.json()
        print(completion)
        return completion["choices"][0]["message"]["content"]
    except Exception as e:
        return f"An error occurred: {e}"


# def makerequest(prompt, request_sdk="dashscope", stream=False, role="user"):
#     try:
#         print("å½“å‰ä½¿ç”¨çš„æ¨¡å‹", option)
#         history = [{"role": "user", "content": prompt}]
#
#
#         if option == "Baichuan2-13B-Chat":
#             openai.api_base = "http://793b3d41.r9.cpolar.top/v1"
#             openai.api_key = "none"
#             model = "Baichuan2-13B-Chat"
#             response = openai.ChatCompletion.create(
#                 model=model, messages=history, temperature=0.95, stream=stream
#             )
#             if stream:
#                 return response
#         elif option == "Qwen1.5-7B-Chat":
#             openai.api_base = "https://856e9b1.r16.vip.cpolar.cn/v1"
#             openai.api_key = "none"
#             model = "/data/lixubin/MiraAgent-server/temp_full/"
#             response = openai.ChatCompletion.create(
#                 model=model, messages=history, temperature=0.95, stream=stream
#             )
#             if stream:
#                 return response
#         elif option == "default":
#             return request_KimiChat(history)
#         elif option == "Qwen1.5-14B":
#             model = "/data/hujinpeng/workspace/Qwen/qwen1.5-14B-Chat-6-14/Qwen1.5-14B-Chat"
#             from openai import OpenAI
#             client = OpenAI(
#                 api_key="none",
#                 base_url="http://6ddbf7f2.r19.cpolar.top/v1"
#             )
#             response = client.chat.completions.create(
#                 model=model, messages=history, temperature=0.1, stream=False
#             )
#             if stream:
#                 return response
#         elif option == "PsycoLLMv1":
#             # openai.api_base = "https://73840636.r27.cpolar.top/v1/"
#             # openai.api_key = "none"
#
#             # model = "/data/hujinpeng/workspace/v2/v2"
#             # response = openai.ChatCompletion.create(
#             #     model=model, messages=history, temperature=0.95, stream=False
#             # )
#             #
#             # 'https://176ead1a.r19.cpolar.top'
#
#             # base_url = 'http://227b84d9.r26.cpolar.top/v1'
#             # model_name = "/data/hujinpeng/workspace/Qwen-14B-sft/all_data/checkpoint-2589"
#
#             base_url = 'http://72554a24.r5.cpolar.cn/v1'
#             model_name = "/data/hujinpeng/workspace/Qwen-32B-sft/QAs+ds+dialogue/checkpoint-474-merge"
#
#             from openai import OpenAI
#             client = OpenAI(
#                 api_key="none",
#                 base_url=base_url
#             )
#             response = client.chat.completions.create(
#                 model=model_name, messages=history, temperature=0.1, stream=False
#             )
#             print('*'*50)
#             print("PsycoLLMv1 used")
#             if stream:
#                 return response
#         elif option == "PsycoLLMv2":
#             # openai.api_base = "http://llmsapi.vip.cpolar.cn/v1"
#             # openai.api_key = "none"
#             # model = "/data/zonepg/models/Qwen/Qwen1.5-32B-LoRA"
#             # response = openai.ChatCompletion.create(
#             #     model=model, messages=history, temperature=0.95, stream=False
#             # )
#             # if stream:
#             #     return response
#
#             model='/data/hujinpeng/workspace/trained_model/checkpoint-1940'
#             from openai import OpenAI
#             client = OpenAI(
#                 api_key="none",
#                 base_url="https://f35fa83.r16.cpolar.top/v1/"
#             )
#             response = client.chat.completions.create(
#                 model=model, messages=history, temperature=0.1, stream=False
#             )
#             print('*' * 50)
#             print("PsycoLLMv2 used")
#             if stream:
#                 return response
#
#
#         else:
#             return request_KimiChat(history)
#
#         res = decode(response.choices[0].message.content)
#         return res
#     except Exception as e:
#         res = "wrong!"
#         print(e)

def makerequest(history, request_sdk="dashscope", stream=False, role="user"):
    try:
        print("å½“å‰ä½¿ç”¨çš„æ¨¡å‹", option)

        if option == "Baichuan2-13B-Chat":
            openai.api_base = "http://793b3d41.r9.cpolar.top/v1"
            openai.api_key = "none"
            model = "Baichuan2-13B-Chat"
            response = openai.ChatCompletion.create(
                model=model, messages=history, temperature=0.95, stream=stream
            )
            if stream:
                return response
        elif option == "Qwen1.5-7B-Chat":
            openai.api_base = "https://856e9b1.r16.vip.cpolar.cn/v1"
            openai.api_key = "none"
            model = "/data/lixubin/MiraAgent-server/temp_full/"
            response = openai.ChatCompletion.create(
                model=model, messages=history, temperature=0.95, stream=stream
            )
            if stream:
                return response
        elif option == "default":
            return request_KimiChat(history)
        elif option == "Qwen1.5-14B":
            model = "/data/hujinpeng/workspace/Qwen/qwen1.5-14B-Chat-6-14/Qwen1.5-14B-Chat"
            from openai import OpenAI
            client = OpenAI(
                api_key="none",
                base_url="http://6ddbf7f2.r19.cpolar.top/v1"
            )
            response = client.chat.completions.create(
                model=model, messages=history, temperature=0.1, stream=False
            )
            if stream:
                return response


        # elif option == "PsycoLLMv1":
        #     # openai.api_base = "https://73840636.r27.cpolar.top/v1/"
        #     # openai.api_key = "none"
        #
        #     # model = "/data/hujinpeng/workspace/v2/v2"
        #     # response = openai.ChatCompletion.create(
        #     #     model=model, messages=history, temperature=0.95, stream=False
        #     # )
        #     #
        #     # 'https://176ead1a.r19.cpolar.top'
        #
        #     # base_url = 'http://227b84d9.r26.cpolar.top/v1'
        #     # model_name = "/data/hujinpeng/workspace/Qwen-14B-sft/all_data/checkpoint-2589"
        #
        #     base_url = 'http://psycollm.vip.cpolar.cn/v1'
        #     model_name = "/data/hujinpeng/workspace/Qwen-32B-sft/QAs+ds+dialogue/checkpoint-474-merge"
        #
        #     from openai import OpenAI
        #     client = OpenAI(
        #         api_key="none",
        #         base_url=base_url
        #     )
        #     response = client.chat.completions.create(
        #         model=model_name, messages=history, temperature=0.1, stream=False
        #     )
        #     print('*'*50)
        #     print("PsycoLLMv1 used")
        #     if stream:
        #         return response

        elif option == "PsycoLLMv1":
            base_url = 'http://psycollm.vip.cpolar.cn/v1'
            model_name = "/data/hujinpeng/workspace/Qwen-32B-sft/QAs+ds+dialogue/checkpoint-474-merge"

            from openai import OpenAI
            client = OpenAI(
                api_key="none",
                base_url=base_url
            )
            response = client.chat.completions.create(
                model=model_name, messages=history, temperature=0.1, stream=False
            )
            print('*' * 50)
            print("PsycoLLMv1 used")
            if stream:
                return response

        elif option == "PsycoLLMv2":
            model = '/data/hujinpeng/workspace/trained_model/checkpoint-1940'
            from openai import OpenAI
            client = OpenAI(
                api_key="none",
                base_url="https://f35fa83.r16.cpolar.top/v1/"
            )
            response = client.chat.completions.create(
                model=model, messages=history, temperature=0.1, stream=False
            )
            print('*' * 50)
            print("PsycoLLMv2 used")
            if stream:
                return response

        else:
            return request_KimiChat(history)

        res = decode(response.choices[0].message.content)
        return res
    except Exception as e:
        res = "wrong!"
        print(e)


with open("chatdemo.json", "rb") as f:
    chatprompt = json.load(f)


def score_prompt_build(text, system_prompt, question):
    """
    text:ç”¨æˆ·å›å¤çš„å†…å®¹
    system_prompt:ç³»ç»Ÿçš„prompt[optional]
    question:æé—®
    """
    return f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢åŠ©æ‰‹ï¼Œè¯·ä¾æ®æ‚£è€…çš„å›ç­”å†…å®¹ï¼Œä»…è¾“å‡ºè¯¥é¡¹å¾—åˆ†ç»“æœã€‚ç”¨æˆ·å›ç­”æ²¡æœ‰ï¼Œè¡¨ç¤ºæ— æ­¤ç—‡çŠ¶ã€‚ã€æé—®å†…å®¹ï¼š{question}\nç”¨æˆ·å›ç­”ï¼š{text}ã€‚ã€‘\næ ¼å¤–æ³¨æ„ï¼šæ ¹æ®ç—‡çŠ¶ç¨‹åº¦åˆ†ä¸ºã€0:æ— ã€1:è½»åº¦ã€2:ä¸­åº¦ã€3:è¾ƒé‡ã€4:é‡åº¦ã€‘ã€‚"


# def question_build(topic, last_topic, last_response, identification):
#     """
#     topic:ç”¨æˆ·å›å¤çš„å†…å®¹
#     system_prompt:ç³»ç»Ÿçš„prompt[optional]
#     """
#     # print(topic,last_topic,last_response)
#     return f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢åŠ©æ‰‹ï¼Œèƒ½å¤Ÿç»“åˆæ±‰å¯†å°”é¡¿ç„¦è™‘é‡è¡¨ä¸ç”¨æˆ·è¿›è¡Œè®¿è°ˆã€‚é¦–å…ˆå¯¹ç”¨æˆ·çš„ä¸Šä¸ªé—®é¢˜çš„å›ç­”è¡¨ç¤ºç†è§£ä¸æ”¯æŒï¼Œå¹¶é’ˆå¯¹{topic}è¿™ä¸€é¡¹ç›®è¿›ä¸€æ­¥è®¿è°ˆæé—®ã€‚æ ¼å¤–æ³¨æ„ï¼šé—®é¢˜æè¿°ç¬¦åˆç”¨æˆ·èº«ä»½ï¼š{identification},æ¯æ¬¡æé—®ä¸è¶…è¿‡150å­—ã€‚ä¸Šä¸ªé—®é¢˜ï¼š{last_topic}ï¼Œç”¨æˆ·å›ç­”ï¼š{last_response}."


def question_build(topic, last_topic, last_response, identification):
    """
    topic:ç”¨æˆ·å›å¤çš„å†…å®¹
    system_prompt:ç³»ç»Ÿçš„prompt[optional]
    """

    # _prompt = f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢åŠ©æ‰‹ï¼Œèƒ½å¤Ÿç»“åˆæ±‰å¯†å°”é¡¿ç„¦è™‘é‡è¡¨ä¸ç”¨æˆ·è¿›è¡Œè®¿è°ˆã€‚é¦–å…ˆå¯¹ç”¨æˆ·çš„ä¸Šä¸ªé—®é¢˜çš„å›ç­”è¡¨ç¤ºç†è§£ä¸æ”¯æŒï¼Œå¹¶é’ˆå¯¹{topic}è¿™ä¸€é¡¹ç›®è¿›ä¸€æ­¥è®¿è°ˆæé—®ã€‚æ ¼å¤–æ³¨æ„ï¼šé—®é¢˜æè¿°ç¬¦åˆç”¨æˆ·èº«ä»½ï¼š{identification},æ¯æ¬¡æé—®ä¸è¶…è¿‡150å­—ã€‚ä¸Šä¸ªé—®é¢˜ï¼š{last_topic}ï¼Œç”¨æˆ·å›ç­”ï¼š{last_response}."

    # _prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢åŠ©æ‰‹ï¼Œé¦–å…ˆå¯¹ç”¨æˆ·çš„ä¸Šä¸ªé—®é¢˜çš„å›ç­”è¡¨ç¤ºç†è§£ä¸æ”¯æŒ,ç»™å‡ºä¸€äº›å…±æƒ…çš„å›å¤ï¼Œ
    #            ä¸Šä¸ªé—®é¢˜çš„ä¸»é¢˜æ˜¯{last_topic}ï¼Œç”¨æˆ·çš„å›ç­”ä¸º{last_response}.
    #            ä¸‹ä¸€æ­¥å°†é’ˆå¯¹{topic}è¿™ä¸€é¡¹ç›®è¿›ä¸€æ­¥è®¿è°ˆæé—®ã€‚
    #            æ ¼å¤–æ³¨æ„ï¼šé—®é¢˜æè¿°ç¬¦åˆç”¨æˆ·èº«ä»½ï¼š{identification},æ¯æ¬¡æé—®ä¸è¶…è¿‡150å­—ã€‚
    #            ä¸‹é¢ç»™å‡ ä¸ªå¯ä»¥æé—®çš„é—®é¢˜ï¼š
    #            {chatprompt[topic][0]}
    #            {chatprompt[topic][1]}
    #            {chatprompt[topic][2]}
    #            """

    _prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢åŠ©æ‰‹å°æºï¼Œèƒ½å¤Ÿå…±æƒ…åœ°ä¸ç”¨æˆ·æ²Ÿé€šã€‚
               ä¸Šä¸ªé—®é¢˜çš„ä¸»é¢˜ï¼š{last_topic}ï¼Œç”¨æˆ·çš„å›ç­”ï¼š{last_response}ã€‚
               é¦–å…ˆå¯¹ç”¨æˆ·çš„ä¸Šä¸ªé—®é¢˜çš„å›ç­”ç»™äºˆä¸€å®šçš„åé¦ˆ,ç»™å‡ºä¸€äº›å…±æƒ…çš„å›å¤ã€‚
               ä¸‹ä¸€æ­¥å°†é’ˆå¯¹{topic}è¿™ä¸€é¡¹ç›®è¿›ä¸€æ­¥è®¿è°ˆæé—®,è¯·æå‡ºé—®é¢˜
               æ ¼å¤–æ³¨æ„ï¼šé—®é¢˜æè¿°ç¬¦åˆç”¨æˆ·èº«ä»½ï¼š{identification},æ¯æ¬¡æé—®ä¸è¶…è¿‡150å­—ã€‚
               ä¾‹å¦‚ä½ å¯ä»¥æå‡ºä»¥ä¸‹é—®é¢˜ï¼š
                {chatprompt[topic][0]}
                {chatprompt[topic][1]}
               """
    # print(topic,last_topic,last_response)
    return _prompt

def single_question_build(topic):
    """
    topic:ç”¨æˆ·å›å¤çš„å†…å®¹
    system_prompt:ç³»ç»Ÿçš„prompt[optional]
    """
    return f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢åŠ©æ‰‹ï¼Œèƒ½å¤Ÿç»“åˆæ±‰å¯†å°”é¡¿ç„¦è™‘é‡è¡¨ä¸ç”¨æˆ·è¿›è¡Œè®¿è°ˆã€‚ç°åœ¨éœ€è¦ä½ é’ˆå¯¹{topic}è¿™ä¸€é¡¹å‘ç”¨æˆ·æé—®ä¸€ä¸ªé—®é¢˜ï¼Œå¹¶æç¤ºç”¨æˆ·è¯¦ç»†æè¿°ç—‡çŠ¶ã€‚æ ¼å¤–æ³¨æ„ï¼šæé—®ä¸è¶…è¿‡150å­—"

# def single_question_build(topic):
#     """
#     topic:ç”¨æˆ·å›å¤çš„å†…å®¹
#     system_prompt:ç³»ç»Ÿçš„prompt[optional]
#     """
#     print('topic',topic)
#     # _prompt = f"""
#     #             ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢åŠ©æ‰‹å°æºï¼Œéœ€è¦äº†è§£ç”¨æˆ·å…³äº{topic}çš„ä¿¡æ¯ï¼Œæ ¹æ®{topic}æé—®ç”¨æˆ·ä¸€ä¸ªç›¸å…³çš„é—®é¢˜ï¼Œç”¨äºäº†è§£ç”¨æˆ·åœ¨{topic}çš„æƒ…å†µã€‚
#     #             æ ¼å¤–æ³¨æ„ï¼šä½ å½“å‰ä¸çŸ¥é“ç”¨æˆ·å…³äº{topic}çš„ä¿¡æ¯ï¼Œæé—®ä¸è¶…è¿‡150å­—
#     #             ä¸‹é¢ç»™å‡ ä¸ªå¯ä»¥æé—®çš„é—®é¢˜ï¼š
#     #             {chatprompt[topic][0]}
#     #             {chatprompt[topic][1]}
#     #             {chatprompt[topic][2]}
#     #             """
#
#     _prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢åŠ©æ‰‹å°æºï¼Œèƒ½å¤Ÿå…±æƒ…åœ°ä¸ç”¨æˆ·æ²Ÿé€šã€‚é’ˆå¯¹{topic}è¿™ä¸€é¡¹ç›®è¿›ä¸€æ­¥è®¿è°ˆæé—®,è¯·æå‡ºé—®é¢˜
#                æ ¼å¤–æ³¨æ„ï¼šæ¯æ¬¡æé—®ä¸è¶…è¿‡150å­—ã€‚
#                 ä¾‹å¦‚ï¼Œä½ å¯ä»¥å‘ä½ çš„ç”¨æˆ·æå‡ºä»¥ä¸‹é—®é¢˜ï¼š
#                 {chatprompt[topic][0]}
#                 {chatprompt[topic][1]}
#                 {chatprompt[topic][2]}
#                 """
#     # _prompt = f"""
#     #             è¯´ä¸€ä¸ªç¬‘è¯
#     #             """
#     return _prompt


def total_score(scores):
    total_score = 0
    # print(scores,len(scores))
    for score in scores:
        numbers = re.findall(r"\d+", score)
        if len(numbers) > 0:
            number = int(numbers[0])
        else:
            number = 0
        total_score += number
    return total_score


def get_number(text):
    numbers = re.findall(r"\d+", text)
    if len(numbers) > 0:
        number = int(numbers[0])
    else:
        number = 0
    return number


def build_conclusion_prompt(responses, questions, topics, system_prompt=""):
    """
    responses:ç”¨æˆ·å›å¤
    questions:é—®é¢˜
    system_prompt:ç³»ç»Ÿprompt
    topics:ä¸»é¢˜
    scores:è¯„åˆ†
    """
    history = ""
    i = 1

    for response, question, topic in tqdm(zip(responses, questions, topics)):
        history += (
            f"å¯¹äºç¬¬{i}ä¸ªé¡¹æœ‰å…³{topic}æ–¹é¢ï¼Œå¿ƒç†åŒ»ç”Ÿæé—®{question},æ‚£è€…å›ç­”{response}"
        )
        i += 1
    return f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢åŠ©æ‰‹ï¼Œè¯·æ ¹æ®å¿ƒç†è®¿è°ˆå¯¹è¯å†å²{history},è¾“å‡ºæˆ‘çš„ä¸»è¿°ç—‡çŠ¶æ‘˜è¦ã€‚"


def build_advice_prompt(responses, questions, topics, system_prompt=""):
    """
    responses:ç”¨æˆ·å›å¤
    questions:é—®é¢˜
    system_prompt:ç³»ç»Ÿprompt
    topics:ä¸»é¢˜
    scores:è¯„åˆ†
    """
    history = ""
    i = 1

    for response, question, topic in tqdm(zip(responses, questions, topics)):
        history += (
            f"å¯¹äºç¬¬{i}ä¸ªé¡¹æœ‰å…³{topic}æ–¹é¢ï¼Œå¿ƒç†åŒ»ç”Ÿæé—®{question},æ‚£è€…å›ç­”{response}"
        )
        i += 1
    return f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢åŠ©æ‰‹ï¼Œè¯·æ ¹æ®å¿ƒç†è®¿è°ˆå¯¹è¯å†å²{history},ç”Ÿæˆä¸ªæ€§åŒ–é’ˆå¯¹æ€§çš„å»ºè®®ã€‚"


def build_table(topics, scores):
    table = "|åºå·|é¡¹ç›®|å¾—åˆ†|\n|----|----|----|\n"
    i = 1
    for topic, score in tqdm(zip(topics, scores)):
        table += f"|{i}|{topic}|{get_number(score)}åˆ†|\n"
        i += 1
    return table


def build_summary_prompt(
    topics, scores, conclusion, system_prompt="", table="", advice=""
):
    """
    responses:ç”¨æˆ·å›å¤
    questions:é—®é¢˜
    system_prompt:ç³»ç»Ÿprompt
    topics:ä¸»é¢˜
    scores:è¯„åˆ†
    """
    scorelist = ""
    i = 1
    totalscore = total_score(scores)
    # print("è¯„åˆ†åˆ—è¡¨ï¼š",scores)

    for topic, score in tqdm(zip(topics, scores)):
        scorelist += f"{i}.{topic}ï¼š{get_number(score)}åˆ†ï¼›\n"
        i += 1
    zhengzhuang = "æ²¡æœ‰ç„¦è™‘"
    if totalscore >= 29:
        zhengzhuang = "ä¸¥é‡ç„¦è™‘"
    elif totalscore >= 21:
        zhengzhuang = "é‡åº¦ç„¦è™‘"
    elif totalscore >= 14:
        zhengzhuang = "ä¸­åº¦ç„¦è™‘"
    elif totalscore >= 7:
        zhengzhuang = "è½»åº¦ç„¦è™‘"
    print(table)

    return f"""# é‡è¡¨

**1. æµ‹è¯„å¾—åˆ†è¡¨:**

{table}

æ€»å¾—åˆ†ï¼š{totalscore}

**2. åˆ†æç»“è®ºï¼š**

æ‚¨çš„æ€»åˆ†æ˜¯ {totalscore}ï¼Œå±äº {zhengzhuang}ï¼Œ{conclusion}

**3. æŠ¥å‘Šå»ºè®®ï¼š**

{advice}
"""


prompt_topics = list(chatprompt.keys())
total_questions = len(prompt_topics)
st.title("ğŸ’¬ Chatbot demo")
# option = st.selectbox(
#     "How would you like to chat with?", ("default", "PsycoLLMv1", "PsycoLLMv2"), index=0
# )

option = st.selectbox(
    "How would you like to chat with?", ("default", "PsycoLLMv1"), index=1
)

if "messages" not in st.session_state:
    question = "æ‚¨å¥½, æˆ‘æ˜¯æ‚¨çš„ä¸“å±å¿ƒç†åŠ©æ‰‹å°æº,å¾ˆé«˜å…´åœ¨è¿™é‡Œä¸ä½ äº¤æµã€‚è¯·æ³¨æ„, è¿™åªæ˜¯ä¸€ä¸ªåˆæ­¥çš„ç­›æŸ¥, ä¸èƒ½ä»£æ›¿æ­£è§„çš„ç²¾ç¥ç§‘è¯Šæ–­å’Œæ²»ç–—ã€‚ é¦–å…ˆä¸ºäº†è¯„ä¼°çš„å‡†ç¡®æ€§ï¼Œæˆ‘æƒ³æ”¶é›†æ‚¨çš„åŸºæœ¬ä¿¡æ¯ï¼šå¹´é¾„ã€æ€§åˆ«ã€èŒä¸šï¼Œå¦‚æœå¯ä»¥ï¼Œæˆ‘ä»¬å¼€å§‹å§ã€‚"


    st.session_state["messages"] = [{"role": "assistant", "content": f"{question}"}]
    st.session_state["questions"] = []
    st.session_state["responses"] = []
    # st.session_state["scores"] = []
    # st.session_state["topics"] = []
    st.session_state["identification"] = ""

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():

    if st.session_state["identification"] == "":
        st.session_state["identification"] = prompt
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)
    else:
        st.session_state["responses"].append(prompt)

    history = st.session_state["messages"]
    response = makerequest(history, stream=False, role="assistant")
    st.chat_message("assistant").write(response)
    st.session_state["messages"].append({"role": "assistant", "content": response})

# if prompt := st.chat_input():
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     st.chat_message("user").write(prompt)
#
#     if (
#         st.session_state["identification"] == ""
#         and len(st.session_state["questions"]) == 0
#     ):
#         st.session_state["identification"] = prompt
#     else:
#         st.session_state["responses"].append(prompt)
#
#     history = st.session_state["messages"]
#     question = makerequest(history, stream=False, role="assistant")
#     st.chat_message("assistant").write(question)
#     st.session_state["questions"].append(question)
#     st.session_state["topics"].append(prompt_topics[len(st.session_state["questions"]) - 1])
#     st.session_state.messages.append({"role": "assistant", "content": f"{question}"})
#
#     if len(st.session_state["questions"]) >= len(prompt_topics):
#         st.chat_message("assistant").write(
#             "æ„Ÿè°¢ä½ çš„çœŸè¯šé…åˆï¼Œæ¥ä¸‹æ¥æˆ‘å°†ä¸ºæ‚¨è¾“å‡ºä¸€ä»½å¿ƒç†åˆç­›æŠ¥å‘Šã€‚ç»“æœä»…ä¾›å‚è€ƒï¼Œä¸ä½œä¸ºåŒ»å­¦è¯Šæ–­ä¾æ®ã€‚ç”ŸæˆæŠ¥å‘Šå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·ç¨ç­‰ã€‚"
#         )
#         history = st.session_state["messages"]
#         summary_response = makerequest(history, stream=False, role="assistant")
#
#         st.session_state.messages.append(
#             {"role": "assistant", "content": f"{summary_response}"}
#         )
#         st.chat_message("assistant").write(summary_response)

        # table = build_table(st.session_state["topics"], st.session_state["scores"])
        # st.session_state.messages.append(
        #     {"role": "assistant", "content": f"{summary_response}"}
        # )
        # st.session_state.messages.append(
        #     {"role": "assistant", "content": table}
        # )
        # st.chat_message("assistant").write(summary_response)
        # st.chat_message("assistant").write(table)



# if prompt := st.chat_input():
#     if (
#         st.session_state["identification"] == ""
#         and len(st.session_state["questions"]) == 0
#     ):
#         st.session_state["identification"] = prompt
#         st.session_state.messages.append({"role": "user", "content": prompt})
#         st.chat_message("user").write(prompt)
#         question_prompt = single_question_build(prompt_topics[0])
#         print('question_prompt',len(st.session_state["questions"]), question_prompt)
#         question = makerequest(question_prompt, stream=False, role="assistant")
#         st.chat_message("assistant").write(question)
#         st.session_state["questions"].append(question)
#         st.session_state["topics"].append(prompt_topics[0])
#         st.session_state.messages.append(
#             {"role": "assistant", "content": f"{question}"}
#         )
#     else:
#         st.session_state.messages.append({"role": "user", "content": prompt})
#         st.chat_message("user").write(prompt)

#         now_index = len(st.session_state["questions"])
#         score_prompt = score_prompt_build(
#             prompt, "", st.session_state["questions"][-1]
#         )  # ç”¨æˆ·å›å¤ä»¥åŠæœ€åä¸€ä¸ªé—®é¢˜
#         score = makerequest(score_prompt)
#         # print("score:",score,"score prompt:",score_prompt)
#         st.session_state["scores"].append(score)
#         st.session_state["responses"].append(prompt)
#
#         if len(st.session_state["questions"]) >= len(prompt_topics):
#             # è¿›è¡Œæ€»ç»“
#             st.chat_message("assistant").write(
#                 "æ„Ÿè°¢ä½ çš„çœŸè¯šé…åˆï¼Œæ¥ä¸‹æ¥æˆ‘å°†ä¸ºæ‚¨è¾“å‡ºä¸€ä»½å¿ƒç†åˆç­›æŠ¥å‘Šã€‚ç»“æœä»…ä¾›å‚è€ƒï¼Œä¸ä½œä¸ºåŒ»å­¦è¯Šæ–­ä¾æ®ã€‚ç”ŸæˆæŠ¥å‘Šå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·ç¨ç­‰ã€‚"
#             )
#             conculsion_prompt = build_conclusion_prompt(
#                 st.session_state["responses"],
#                 st.session_state["questions"],
#                 st.session_state["topics"],
#             )
#             conclusion = makerequest(conculsion_prompt)
#             table = build_table(st.session_state["topics"], st.session_state["scores"])
#             advice_prompt = build_advice_prompt(
#                 st.session_state["responses"],
#                 st.session_state["questions"],
#                 st.session_state["topics"],
#             )
#             advice = makerequest(advice_prompt)
#             summary_prompt = build_summary_prompt(
#                 st.session_state["topics"],
#                 st.session_state["scores"],
#                 conclusion,
#                 "",
#                 advice=advice,
#                 table=table,
#             )
#             # query_res = makerequest(summary_prompt,stream=False,role="assistant")
#             # print(summary_prompt)
#             query_res = summary_prompt
#             st.session_state.messages.append(
#                 {"role": "assistant", "content": f"{query_res}"}
#             )
#             st.chat_message("assistant").write(query_res)
#             # print(st.session_state["messages"])
#         else:
#
#             question_prompt = question_build(
#                 prompt_topics[now_index],
#                 st.session_state["topics"][-1],
#                 st.session_state["responses"][-1],
#                 st.session_state["identification"],
#             )
#             print('question_prompt', len(st.session_state["questions"]), question_prompt)
#             question = makerequest(question_prompt, stream=False, role="assistant")
#             st.chat_message("assistant").write(question)
#             st.session_state["questions"].append(question)
#             st.session_state["topics"].append(prompt_topics[now_index])  # è¿™é‡Œæ›´æ–°topic
#             st.session_state.messages.append(
#                 {"role": "assistant", "content": f"{question}"}
#             )




audio = AudioSegment.empty()
audio = audiorecorder(
    start_prompt="Start recording",
    stop_prompt="Stop recording",
    pause_prompt="",
    show_visualizer=True,
    key=None,
)
print("ceshi0.1")
print(type(audio))
print(len(audio))
audio_len = read_audio_len(file_path)
print(audio_len)
if len(audio) > 0 and len(audio) != audio_len:
    print("ceshi1")
    audio.export("audio.wav", format="wav", parameters=["-ar", "16000"])
    print("ceshi1.1")

    result, status = speech2text("audio.wav")
    edited_text = st.text_area(
        "Edit the transcribed text before submitting:",
        result,
        label_visibility="collapsed",
    )

    if st.button("Submit"):
        prompt = edited_text
        edited_text = None
        result = None
        audio_len = len(audio)
        write_audio_len(file_path, audio_len)
        print("ceshi2")
        if (
            st.session_state["identification"] == ""
            and len(st.session_state["questions"]) == 0
        ):
            st.session_state["identification"] = prompt
            st.session_state.messages.append({"role": "user", "content": prompt})
            st.chat_message("user").write(prompt)
            question_prompt = single_question_build(prompt_topics[0])
            # print(question_prompt)
            question = makerequest(question_prompt, stream=False, role="assistant")
            st.chat_message("assistant").write(question)
            st.session_state["questions"].append(question)
            st.session_state["topics"].append(prompt_topics[0])
            st.session_state.messages.append(
                {"role": "assistant", "content": f"{question}"}
            )
        else:
            st.session_state.messages.append({"role": "user", "content": prompt})
            st.chat_message("user").write(prompt)
            now_index = len(st.session_state["questions"])
            score_prompt = score_prompt_build(
                prompt, "", st.session_state["questions"][-1]
            )  # ç”¨æˆ·å›å¤ä»¥åŠæœ€åä¸€ä¸ªé—®é¢˜
            score = makerequest(score_prompt)
            # print("score:",score,"score prompt:",score_prompt)
            st.session_state["scores"].append(score)
            st.session_state["responses"].append(prompt)

            if len(st.session_state["questions"]) >= len(prompt_topics):
                # è¿›è¡Œæ€»ç»“
                st.chat_message("assistant").write(
                    "æ„Ÿè°¢ä½ çš„çœŸè¯šé…åˆï¼Œæ¥ä¸‹æ¥æˆ‘å°†ä¸ºæ‚¨è¾“å‡ºä¸€ä»½å¿ƒç†åˆç­›æŠ¥å‘Šã€‚ç»“æœä»…ä¾›å‚è€ƒï¼Œä¸ä½œä¸ºåŒ»å­¦è¯Šæ–­ä¾æ®ã€‚ç”ŸæˆæŠ¥å‘Šå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·ç¨ç­‰ã€‚"
                )
                conculsion_prompt = build_conclusion_prompt(
                    st.session_state["responses"],
                    st.session_state["questions"],
                    st.session_state["topics"],
                )
                conclusion = makerequest(conculsion_prompt)
                table = build_table(
                    st.session_state["topics"], st.session_state["scores"]
                )
                advice_prompt = build_advice_prompt(
                    st.session_state["responses"],
                    st.session_state["questions"],
                    st.session_state["topics"],
                )
                advice = makerequest(advice_prompt)
                summary_prompt = build_summary_prompt(
                    st.session_state["topics"],
                    st.session_state["scores"],
                    conclusion,
                    "",
                    advice=advice,
                    table=table,
                )
                # query_res = makerequest(summary_prompt,stream=False,role="assistant")
                # print(summary_prompt)
                query_res = summary_prompt
                st.session_state.messages.append(
                    {"role": "assistant", "content": f"{query_res}"}
                )
                st.chat_message("assistant").write(query_res)
                # print(st.session_state["messages"])
            else:

                question_prompt = question_build(
                    prompt_topics[now_index],
                    st.session_state["topics"][-1],
                    st.session_state["responses"][-1],
                    st.session_state["identification"],
                )

                question = makerequest(question_prompt, stream=False, role="assistant")
                st.chat_message("assistant").write(question)
                st.session_state["questions"].append(question)
                st.session_state["topics"].append(
                    prompt_topics[now_index]
                )  # è¿™é‡Œæ›´æ–°topic
                st.session_state.messages.append(
                    {"role": "assistant", "content": f"{question}"}
                )
    else:
        st.write(result)


# questions = []
# responses = []
# scores = []
# topics = []

# for key in list(chatprompt.keys()):
#     question_prompt = question_build(key,chatprompt[key])
#     question = makerequest(question_prompt)
#     topics.append(key)
#     print("AI: ",question)
#     response = input("Human: ")
#     score_prompt = score_prompt_build(response,"",question)
#     # print(score_prompt)
#     score = makerequest(score_prompt)
#     print("score:",score)
#     questions.append(question)
#     responses.append(response)
#     scores.append(score)
# summary_prompt = build_summary_prompt(responses,questions,topics,scores,"")
# print(makerequest(summary_prompt))
